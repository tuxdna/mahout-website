Title: ClusteringYourData
+*Mahout_0.8*+

After you've done the [Quickstart](quickstart.html)
 and are familiar with the basics of Mahout, it is time to cluster your own
data. 

The following pieces *may* be useful for in getting started:

<a name="ClusteringYourData-Input"></a>
# Input

For starters, you will need your data in an appropriate Vector format
(which has changed since Mahout 0.1)

* See [Creating Vectors](creating-vectors.html)

<a name="ClusteringYourData-TextPreparation"></a>
## Text Preparation

* See [Creating Vectors from Text](creating-vectors-from-text.html)
*
http://www.lucidimagination.com/search/document/4a0e528982b2dac3/document_clustering

<a name="ClusteringYourData-RunningtheProcess"></a>
# Running the Process

<a name="ClusteringYourData-Canopy"></a>
## Canopy

Background: [canopy ](-canopy-clustering.html)

Documentation of running canopy from the command line: [canopy-commandline](canopy-commandline.html)

<a name="ClusteringYourData-kMeans"></a>
## kMeans

Background: [K-Means Clustering](k-means-clustering.html)

Documentation of running kMeans from the command line: [k-means-commandline](k-means-commandline.html)

Documentation of running fuzzy kMeans from the command line: [fuzzy-k-means-commandline](fuzzy-k-means-commandline.html)

<a name="ClusteringYourData-Dirichlet"></a>
## Dirichlet

Background: [dirichlet ](-dirichlet-process-clustering.html)

Documentation of running dirichlet from the command line: [dirichlet-commandline](dirichlet-commandline.html)

<a name="ClusteringYourData-Mean-shift"></a>
## Mean-shift

Background:  [meanshift ](-mean-shift-clustering.html)

Documentation of running mean shift from the command line: [mean-shift-commandline](mean-shift-commandline.html)

<a name="ClusteringYourData-LatentDirichletAllocation"></a>
## Latent Dirichlet Allocation

Background and documentation: [LDA](-latent-dirichlet-allocation.html)

Documentation of running LDA from the command line: [lda-commandline](lda-commandline.html)

<a name="ClusteringYourData-RetrievingtheOutput"></a>
# Retrieving the Output

Mahout has a cluster dumper utility that can be used to retrieve and
evaluate your clustering data.

    ./bin/mahout clusterdump <OPTIONS>


<a name="ClusteringYourData-Theclusterdumperoptionsare:"></a>
## The cluster dumper options are:

      --help (-h)				   Print out help		    
      --input (-i) input			   The directory containing
Sequence    
    					   Files for the Clusters	    
      --output (-o) output			   The output file.  If not
specified,  
    					   dumps to the console.
      --outputFormat (-of) outputFormat	   The optional output format to
write
    					   the results as. Options: TEXT,
CSV, or GRAPH_ML		 
      --substring (-b) substring		   The number of chars of the	    
    					   asFormatString() to print	    
      --pointsDir (-p) pointsDir		   The directory containing points  
    					   sequence files mapping input
vectors 
    					   to their cluster.  If specified, 
    					   then the program will output the 
    					   points associated with a cluster 
      --dictionary (-d) dictionary		   The dictionary file. 	    
      --dictionaryType (-dt) dictionaryType    The dictionary file type	    
    					   (text|sequencefile)
      --distanceMeasure (-dm) distanceMeasure  The classname of the
DistanceMeasure.
    					   Default is SquaredEuclidean.     
      --numWords (-n) numWords		   The number of top terms to print 
      --tempDir tempDir			   Intermediate output directory
      --startPhase startPhase		   First phase to run
      --endPhase endPhase			   Last phase to run
      --evaluate (-e)			   Run ClusterEvaluator and
CDbwEvaluator over the
    					   input. The output will be
appended to the rest of
    					   the output at the end.   


More information on using clusterdump utility can be found [here](cluster-dumper.html)

<a name="ClusteringYourData-ValidatingtheOutput"></a>
# Validating the Output

From Ted Dunning's response on See
http://www.lucidimagination.com/search/document/dab8c1f3c3addcfe/validating_clustering_output
{quote}
A principled approach to cluster evaluation is to measure how well the
cluster membership captures the structure of unseen data.  A natural
measure for this is to measure how much of the entropy of the data is
captured by cluster membership.  For k-means and its natural L_2 metric,
the natural cluster quality metric is the squared distance from the nearest
centroid adjusted by the log_2 of the number of clusters.  This can be
compared to the squared magnitude of the original data or the squared
deviation from the centroid for all of the data.  The idea is that you are
changing the representation of the data by allocating some of the bits in
your original representation to represent which cluster each point is in. 
If those bits aren't made up by the residue being small then your
clustering is making a bad trade-off.

In the past, I have used other more heuristic measures as well.  One of the
key characteristics that I would like to see out of a clustering is a
degree of stability.  Thus, I look at the fractions of points that are
assigned to each cluster or the distribution of distances from the cluster
centroid. These values should be relatively stable when applied to held-out
data.

For text, you can actually compute perplexity which measures how well
cluster membership predicts what words are used.  This is nice because you
don't have to worry about the entropy of real valued numbers.

Manual inspection and the so-called laugh test is also important.  The idea
is that the results should not be so ludicrous as to make you laugh.
Unfortunately, it is pretty easy to kid yourself into thinking your system
is working using this kind of inspection.  The problem is that we are too
good at seeing (making up) patterns.
{quote}


<a name="ClusteringYourData-References"></a>
# References

* [Mahout archive references](http://www.lucidimagination.com/search/p:mahout?q=clustering)
